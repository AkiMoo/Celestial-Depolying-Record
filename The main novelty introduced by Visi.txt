The main novelty introduced by Vision GNN architecture (ViG) [15] is the decoupling of the computational graph from
the image’s regular grid structure and sequential structures adopted by CNN and transformer-based architectures. Instead,
ViG splits the input image into several patches and projects them into a high-dimensional embedding space. The computational graph, i.e., the adjacency matrix adopted for the message passing in graph neural networks, is dynamically computed with a configurable number of neighbors based on similarity between embeddings. Each individual patch is treated as a node in a directed graph, with directed edges being created between the patch itself and the top-k most similar patches based on their embeddings. 

related works


计算机视觉中的主流网络架构过去通常是卷积网络[29, 27, 17]。从LeNet [29]开始，CNN在各种视觉任务中取得了成功应用，例如图像分类[27]、物体检测[42]和语义分割[36]。在过去的十年中，CNN架构发展迅速。代表性的工作包括ResNet [17]、MobileNet [21]和NAS [75, 70]。Vision Transformer是从2020年开始为视觉任务引入的[14, 9, 3, 4]。从那时起，提出了许多ViT [9]的变体以改善视觉任务的性能。主要改进包括金字塔架构[57, 35]、局部注意力[15, 35]和位置编码[61]。在Vision Transformer的启发下，MLP也在计算机视觉中得到了探索[49, 50]。通过特别设计的模块[5, 32, 12, 48]，MLP可以实现有竞争力的性能，并且适用于对象检测和分割等通用视觉任务。

2.2 图神经网络
最早的图神经网络最初在[11, 44]中概述。Micheli [38]通过组合非递归层构建了早期的基于空间的图卷积网络。近几年，基于空间的GCN的变体已经被引入，例如[39, 1, 10]。基于谱的GCN最早由Bruna等人提出[2]，它基于谱图理论引入了图卷积。自那时以来，已经提出了许多改进和扩展基于谱的GCN的工作[18, 7, 26]。GCN通常应用于图数据，例如社交网络[13]、引用网络[45]和生物化学图[55]。

GCN在计算机视觉领域的应用[63, 28, 56, 25]主要包括点云分类、场景图生成和动作识别。点云是空间中一组3D点，通常通过LiDAR扫描收集。GCN已经用于对点云进行分类和分割[28, 58, 69]。场景图生成旨在将输入图像解析成包含物体及其关系的图，通常通过结合物体检测器和GCN来解决[63, 68]。通过处理自然形成的关节连接图，GCN被用于人体动作识别任务[24, 67]。GCN只能处理具有自然构建图的特定视觉任务。对于计算机视觉中的一般应用，我们需要一个基于GCN的主干网络，直接处理图像数据。

Once the graph has been defined, information is processed by a neural message passing mechanism, namely Grapher layer, consisting of maxrelative graph convolution [36] with a 2-layer MLP with ReLU nonlinearities. 

RCB和RTCB部分
Figure2显示了语义压缩过程的架构，它由以下两个模块组成：残差卷积块（RCB）和残差转置卷积块（RTCB），残差结构有助于增强联合源通道编码能力。残差网络由卷积，反卷积层和PReLU激活函数构成，在卷积或转置卷积层之后使用的归一化操作是基于广义归一化变换（GDN）的局部除法归一化。在处理图像压缩和密度建模任务中已证明非常有效[29]。卷积或转置卷积层由参数m × n | s指定，其中m和n对应于使用的卷积核的宽度、高度，符号↓和↑表示下采样操作和上采样操作，参数s表示步长长度。本文中使用 5 × 5 的卷积核，采样步长为2。通过这样处理，能够有效压缩卫星发送端的数据量，同时一定程度保证地面站能通过收到的数据信息还原出图结构的特征。地面站接收到语义信息，然后通过像素上采样模块对特征信息升维；最后将与图结构维度相同的特征张量送入已经训练好的推理模块进行图像推理。







Vision GNN架构（ViG）[15]引入的主要创新点是将将输入图像分割成多个图块，并将它们投影到高维嵌入空间中。这样，就将图像的常规网格结构的顺序与旧有的框架解耦合（比如CNN以及基于Transformer的架构）。对于一个大小为 H × W × Channel 的图像，我们将其分割成 N 个图块。通过将每个图块转换为一个特征向量 xi ∈ ℝ^D，我们得到 X = [x₁, x₂, ..., x_N]，其中 D 是特征维度，i = 1, 2, ..., N 表示图块的索引。对于每个节点 v_i，我们找到其 K 个最近邻节点 N(v_i)，并在每个 v_j ∈ N(v_i) 上添加一条从 v_j 指向 v_i 的有向边 e_ji。这样我们就得到了一个图 G = (V, E)，其中 E 表示所有的边。计算图，即在图神经网络中用于消息传递的邻接矩阵，是根据嵌入之间的相似性动态计算的，并且可以根据需要配置邻居数量。每个图块被视为有向图中的一个节点，根据它们的嵌入，与自身和与之最相似的前k个图块之间建立有向边。

位置编码。为了表示节点的位置信息，我们向每个节点特征添加一个位置编码向量：

xi = xi + ei (8)

其中ei是一个维度为D的向量。绝对位置编码如公式8所描述的方式应用于各向同性和金字塔架构。对于金字塔ViG，我们还采用了类似Swin Transformer [35]的先进设计，包括相对位置编码。对于节点i和j，它们之间的相对位置距离是eTiej，这将被添加到特征距离中以构建图形。

其中，F Cnb 和 F Cb 分别表示没有偏置项和具有偏置项的两个全连接层，σ 表示 ReLU 非线性激活函数。z 和 e 表示 Grapher 学习到的图块嵌入，在前馈神经网络（FFN）模块投影之前和投影之后的状态，其中 x_h_i 是输入图块的嵌入。

ViG编码器由一系列的Grapher模块和卷积层组成，每个卷积层将输入的分辨率减半，即在每个Grapher模块和卷积模块之后，图块的数量减少了4倍。值得注意的一点是，ViG根据输入图像和图块嵌入动态地构建图。每个图块被视为输入图中的一个节点，由其图块嵌入来描述。ViG在每个Grapher层自动构建一个不同的图（在我们的修订模型中，构建了三个不同的图，由于存在下采样模块，每个图的节点数也不同），通过计算每个图块与其他所有图块之间的距离来完成。给定预定义的邻居数 K，每个图块与其 K 个最近邻图块以有向方式相连。更具实际意义的是，给定两个输入图像 xi 和 xj，以及相同的图块 h xh_i、xh_j，与图块 xh_i 相邻的图块与图块 xh_j 的邻居可能在不同的位置上。图块的位置不直接参与边的创建，位置信息仅通过图块嵌入间接地间接考虑。示例如图1所示。

作者还提出了一种变体，即金字塔式的 PyramidViG，它构建金字塔特征并综合提取的图块的多尺度特性，提高了模型的性能[15]。在我们的实验评估中，我们实现了 PyramidViG 架构，并在第四部分中简称为 ViG。金字塔架构。金字塔架构通过在层次深入的过程中逐渐提取具有较小空间大小的特征来考虑图像的多尺度特性，例如ResNet [17] 和PVT [57]。经验证据表明，金字塔架构对于视觉任务是有效的[57]。因此，我们采用了这种先进的设计，构建了四个金字塔ViG模型。详细信息如表2所示。需要注意的是，在前两个阶段中，我们采用了空间缩减[57]的方法来处理大量节点。

金字塔ViG（Pyramid ViG）。金字塔架构随着网络的加深逐渐缩小特征图的空间大小，利用图像的尺度不变性产生多尺度特征。先进的网络通常采用金字塔架构，如ResNet [17]、Swin Transformer [35]和CycleMLP [5]。我们在表5中将金字塔ViG与这些代表性的金字塔网络进行了比较。我们的金字塔ViG系列在性能上能够超越或与最先进的金字塔网络（包括CNN、MLP和Transformer）相媲美。这表明图神经网络在视觉任务上表现出色，并有潜力成为计算机视觉系统中的基本组件。



最后，图像分类是在 ViG 编码器的顶部使用一个简单分类头完成的，该分类头由一个池化层和 MLP 分类器组成。


由于本研究采用的其中一个基准数据集的输入图像分辨率较低（120x120像素），为了避免模型崩溃到单个图块上，我们对 ViG 架构进行了修订。这是因为在每个 Grapher 层之后，中间表示的高度和宽度减半，每一步将图块数量减少了4倍，导致最后一个 Grapher 层中图块数量非常少。为了解决这个问题，我们修改了 ViG 的编码器结构，使其具有与原始实现相似数量的可学习参数，并避免了图块数量方面的模型崩溃。具体而言，我们将 Grapher 的阶段数从 4 减少到 3，分别使用维度为 [128, 256, 512] 的嵌入空间，而不是原始实现中的 [48, 96, 240, 384]（ViG-Tiny）。此外，我们将头的数量从 4 增加到 16，并在每个 Grapher 层的图形创建中保持 K = 9 的层数。由于多光谱成像中输入通道和输入分辨率的变化，每个数据集的模型大小在第四部分中介绍，并有所不同。结果的 ViG 架构示意图如图2所示。

ViG在三个开放的土地覆盖分类基准数据集上进行了评估：两个多类别分类数据集和一个多标签分类数据集。前两个数据集是RESISC45 [18] 和 PatternNet [19]，都是由分辨率为256x256像素的仅RGB获取的数据。这两个数据集使用Google Earth收集，分别提供了31500和30400张卫星图像。RGB数据集完全平衡，分别包含45个和38个不同的类别，每个类别分别包含700和800个样本。RESISC45 数据集的分辨率范围从每像素0.2到30米，而PatternNet的分辨率范围从每像素6到50厘米。RESISC45 数据集的获取涵盖了100多个国家，图像条件在天气、地形形态特征和光照方面具有很高的变异性。后一个数据集是一个大规模的多标签图像场景分类数据集，即BigEarthNet [16]，[17]，由590326个 Sentinel-2 L2A 和 Sentinel-1 获取的数据（VV 和 VH 极化）组成，总共提供14个通道。在我们的实验中，我们限制了对 Sentinel-2 的12个波段进行分析。BigEarthNet 中最高分辨率的通道尺寸为120x120像素，空间分辨率从每像素60米到10米不等，具体取决于光谱波段。较低分辨率的获取数据使用双线性插值上采样以匹配最高分辨率，获得了所有分辨率为120x120像素的输入图像。BigEarthNet 数据集提供了两个高度不平衡的标签集：一个较简单的任务由19个标签组成，一个较难的任务由43个不同的类别组成。我们采用了较难的标签集，仅考虑了更具挑战性的任务。所有数据集都被分成了训练、验证和测试集。对于BigEarthNet，我们采用了原始作者使用的划分方式。对于RESISC45，我们选择了与 [37] 中相同的划分方式，而对于PatternNet，我们使用了70/15/15的随机非重叠划分。表I总结了不同数据集的特征。

由于BigEarthNet存在类别不平衡问题，我们针对每个实验报告了微平均的F1-Score、召回率、精确度和准确度。而对于RESISC45和PatternNet数据集，由于它们具有类别平衡的分布，报告了宏平均分数。

我们将ViG的性能与另外两个模型进行了比较：ResNet和ViT。具体而言，我们选择了ViG-Tiny模型，根据原始论文的描述，它有7.1M个可训练参数。类似地，我们选择了ResNet18和ViT-Tiny模型进行比较（分别为11M和5.5M），这是因为相对于ViG-Tiny，ResNet34、ViT-Small和ViT-Base的大小较大（分别为21.7M、22M和86M）。模型大小与数据集的比较如表II所示。

ViG-Tiny在BigEarthNet和RESISC45之间可训练参数的数量差异归因于两个原因：（i）输入通道的数量不同（12 vs. 3），以及（ii）表II中后两个数据集的更高输入分辨率（以像素为单位），这导致了更多的参数分配给可学习的位置编码向量。

我们选择了AdamW优化器，并为所有数据集和模型的训练过程设置了学习率为10^-4的参数。权重衰减的范围在10^-2和10^-4之间，并且最多进行100个epochs。此外，我们采用了早停机制，通过监控验证损失来进行，如果连续10个epochs的验证损失没有改善，就会减小学习率的因子为10，并设置了5个epochs的耐心期。训练过程中使用交叉熵损失进行优化。训练超参数的摘要如表III所示。

在本节中，我们比较了三个模型在三个数据集上进行的结果，分别是多类别分类（两个数据集）和多标签分类（一个数据集）。由于两种情况下的类别平衡度不同，我们在第一种情况下评估了宏平均指标，而在后一种情况下评估了微平均分数。

多类别分类：本文考虑的多类别分类问题代表了中等规模数据集的较简单任务。表IV显示了宏平均的F1、精确度和召回率分数。此外，我们还计算了每个模型的每类指标的最大值和最小值。
从PatterNet上获得的结果可以看出，所考虑的图像分类任务相对于RESISC45来说难度较低。所有模型的得分都高于99.0，其中ViT是三者中最差的。ResNet和ViG的F1、精确度和召回率分数相似，而ViT的指标低于0.63。然而，在评估最小每类情况时，ViG的最小召回率为97.62，比ViT高出5.06个百分点（pp），比ResNet高出0.89pp。综合考虑所有指标，视觉GNN模型在所有指标上取得了最好的成绩，除了最小精确度指标，ResNet在该指标上表现优于ViG。

实验结果实际上表明，在解决RESISC45中提出的分类任务时，存在更高的复杂性，ViG模型获得了最大平均F1分数为86.34，比ViT和ResNet分别高出10.27pp和2.65pp。在考虑最低每类指标时，ViG在F1、精确度和召回率方面分别比ViT高出17.44pp、13.64pp和21.91pp，差距进一步增大。

总体而言，基于图结构学习图像表示在中等规模数据集的土地覆盖分类中表现出了最先进的性能，从而证实了基于Transformer的架构的主要限制，即需要大量数据来学习更好的图像表示。

多标签分类：在BigEarthNet数据集上的实验证实了ViG架构在多光谱成像和遥感方面的学习能力（包括RGB、红外和超蓝信息的12个输入通道）。在这种情况下，考虑到所有三个微平均指标，ViG表现出更好的性能，其次是ViT（F1提升了0.21pp），而ResNet则表现最差（提升了2.85pp），尽管它的可训练参数数量较少，相较于IV-E1节中的实验。这是由于图像中补丁数量显著减少，因此与位置编码张量相关联的参数数量也减少了。




将图像视为图数据的优势包括：1) 图是一种广义的数据结构，网格和序列可以被视为图的特例；2) 图相比于网格或序列更灵活，可以更好地建模复杂对象，因为图像中的对象通常不是正方形，形状是不规则的；3) 一个对象可以被视为由多个部分组成（例如，一个人可以粗略地分为头部、上半身、手臂和腿部），而图结构可以构建这些部分之间的连接；4) 对GNN的先进研究成果可以应用于解决视觉任务。




